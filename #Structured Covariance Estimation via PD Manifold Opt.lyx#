#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass IEEEtran
\begin_preamble
\usepackage{amssymb}
\def\m{\mathbf}
\end_preamble
\options onecolumn
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Structured Covariance Estimation via HPD Manifold Optimization
\end_layout

\begin_layout Author
Huanyu Zhang
\end_layout

\begin_layout Abstract
This paper considers the problem of estimating a structured covariance matrix
 with zero mean.
 This problem can always be formulated to optimize the maximum likelihood
 (ML) problem under the hermitian positive definite (HPD) constraint.
 First, we find that the ML estimator is geodesic convex (g-convex) which
 allows global optimization.
 Therefore, lots of convex optimization algorithms on manifolds can be applied
 to this problem.
 We will apply many manifold optimization methods to handle this problem.
\end_layout

\begin_layout Abstract
On the other hand, we choose to build covariance family by connecting representa
tive covariance matrices (called 
\begin_inset Quotes eld
\end_inset

anchors
\begin_inset Quotes erd
\end_inset

) through geodesics.
 The resulting covariance classes can thus be tailored to the problem of
 interest.
 Second, as an alternative to maximizing the likelihood, we also advocate
 for a differential geometric approach to estimation: minimizing the geodesic
 distance, which can be understood as a projection.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Estimating the covariance matrix is a common problem that is critically
 important in many fields such as signal processing, wireless communications,
 bioinformatics, and financial engineering.
 
\end_layout

\begin_layout Standard
Classical covariance estimation has two basic assumptions: the number of
 samples is larger than the dimensions of the samples; the samples are drawn
 from an Gaussian distribution.
 However, in practice, there are many applications holds neither of these
 assumptions.
\end_layout

\begin_layout Standard
For the situation of high dimension covariance estimation using small number
 of samples, one kind of classical approaches is adding penalties to the
 cost function which can exploit prior knowledge of the unknown parameters,
 such as scalar penalties 
\begin_inset CommandInset citation
LatexCommand cite
key "wang_maximum_2006"
literal "false"

\end_inset

, matrix penalties 
\begin_inset CommandInset citation
LatexCommand cite
key "wiesel_unified_2012"
literal "false"

\end_inset

, and smoothness penalties 
\begin_inset CommandInset citation
LatexCommand cite
key "bucciarelli_optimum_1996"
literal "false"

\end_inset

.
 Another approaches is the shrinkage technique, which includes shrinkage
 to identity 
\begin_inset CommandInset citation
LatexCommand cite
key "chen_shrinkage_2010"
literal "false"

\end_inset

, shrinkage to diagonal structure 
\begin_inset CommandInset citation
LatexCommand cite
key "schafer_shrinkage_2005"
literal "false"

\end_inset

, and shrinkage to positive definite matrix 
\begin_inset CommandInset citation
LatexCommand cite
key "wiesel_unified_2012"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
For the situation of samples from the distribution except for Gaussian distribut
ion.
 An alternative to maximizing the likelihood is proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "musolas2020covariance"
literal "false"

\end_inset

, which does not require assuming a particular probability distribution
 for the data.
 Another common study 
\begin_inset CommandInset citation
LatexCommand cite
key "tyler_distribution-free_1987"
literal "false"

\end_inset

 is to replace the Gaussian assumption with a more general scaled Gaussian
 model, e.g., elliptical distributions.
 Tyler's ML estimator 
\begin_inset CommandInset citation
LatexCommand cite
key "pascal_covariance_2008"
literal "false"

\end_inset

, a well-studied estimator, performs robustly in scaled Gaussian distribution,
 the Tyler's estimator defined as the solution to the fixed-point equation
\begin_inset Formula 
\begin{equation}
\mathbf{R}^{(k+1)}=\frac{K}{N}\sum_{i=1}^{N}\frac{\mathbf{x}_{i}\mathbf{x}_{i}^{T}}{\mathbf{x}_{i}^{T}\left(\mathbf{R}^{(k)}\right)^{-1}\mathbf{x}_{i}}
\end{equation}

\end_inset

is a minimax estimator.
 On the other hand, Tyler's ML estimator can also combine with the shrinkage
 techniques 
\begin_inset CommandInset citation
LatexCommand cite
key "wiesel_unified_2012"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In many scenarios, the covariance matrix has special structure naturally,
 which implies a reduction in the number of parameters to be estimated,
 and thus is beneficial to improving the estimation accuracy.
 For example, Kronecker structures, also known as separable models, transposable
 covariances models, or matrix-variate-normal models are typically used
 when dealing with random matrices.
 Covariance matrix estimation in Kronecker structures involves a non-convex
 optimization.
 But its maximum likelihood estimation function is g-convex 
\begin_inset CommandInset citation
LatexCommand cite
key "wiesel_geodesic_2012"
literal "false"

\end_inset

.
 It is concluded that any local minimum of the cost function on a group
 Kronecker constraint set is a global minimum.
 Many numerical algorithms such as the majorization-minimization algorithm
 is also proposed to solve the constrained minimization problem 
\begin_inset CommandInset citation
LatexCommand cite
key "sun_robust_2016"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Problem Formulation
\end_layout

\begin_layout Standard
Suppose a column vector 
\begin_inset Formula $x$
\end_inset

 is drawn from an 
\begin_inset Formula $N$
\end_inset

-dimensional Gaussian distribution with zero mean and covariance matrix
 
\begin_inset Formula $\m R$
\end_inset

, the corresponding probability density function is 
\begin_inset CommandInset citation
LatexCommand cite
key "burg_estimation_1982"
literal "false"

\end_inset


\begin_inset Formula 
\begin{equation}
p(\m x)=(2\pi)^{-N/2}\det(\m R)^{-1/2}e^{-\frac{\m x^{T}\m R^{-1}\m x}{2}}.\label{eq:probability density function}
\end{equation}

\end_inset

Now, instead of a single vector sample, suppose that we have 
\begin_inset Formula $M$
\end_inset

 independent vector samples, 
\begin_inset Formula $\m x_{i}$
\end_inset

, 
\begin_inset Formula $i=1$
\end_inset

 to 
\begin_inset Formula $M$
\end_inset

.
 The 
\color magenta
probability density
\color inherit
 for this set of vectors follows from (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:probability density function"
plural "false"
caps "false"
noprefix "false"

\end_inset

) as
\begin_inset Formula 
\begin{equation}
p(\m x_{1},\m x_{2},\ldots,\m x_{N})=(2\pi)^{-NM/2}\det(\m R)^{-M/2}e^{-\sum_{i=1}^{M}\frac{\m x_{i}^{T}\m R^{-1}\m x_{i}}{2}}.\label{eq:maximaize}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The 
\begin_inset Formula $R$
\end_inset

 that has a special structure and maximizes (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:maximaize"
plural "false"
caps "false"
noprefix "false"

\end_inset

) is the 
\begin_inset Quotes eld
\end_inset

maximum-likelihood
\begin_inset Quotes erd
\end_inset

 estimate of the covariance matrix.
 In order to simplify the equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:maximaize"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we take the logarithm of it, we get
\begin_inset Formula 
\begin{equation}
-\frac{MN}{2}\log(2\pi)-\frac{M}{2}\log\det(\mathbf{R})-\frac{1}{2}\sum_{i=1}^{M}\m x_{i}^{T}\mathbf{R}^{-1}\m x_{i}.
\end{equation}

\end_inset

 Dropping the leading constant term and dividing by 
\begin_inset Formula $M/2,$
\end_inset

 we define our optimization problem to be
\begin_inset Formula 
\begin{equation}
\begin{aligned} & \mathop{\text{minimize}}_{\mathbf{R}} &  & \frac{1}{M}\log\det(\mathbf{R})+\sum_{i=1}^{M}\log(\m x_{i}^{T}\mathbf{R}^{-1}\m x_{i})\\
 & \mathop{\text{subject to}} &  & \mathbf{R}\in\text{HPD.}
\end{aligned}
\label{eq:optimization}
\end{equation}

\end_inset

The former part in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:optimization"
plural "false"
caps "false"
noprefix "false"

\end_inset

) is concave.
 We first restrict the function 
\begin_inset Formula $\log\det(\m R)$
\end_inset

 to a line, let 
\begin_inset Formula $g(t)=\log\det(\m R+t\m V)$
\end_inset

.
 We can check the convexity of 
\begin_inset Formula $\log\det(\m R)$
\end_inset

 by checking the convexity of 
\begin_inset Formula $g(t)$
\end_inset


\begin_inset Formula 
\begin{equation}
\begin{aligned}g(t) & =\log\det(\m R)+\log\det(\m I+t\m R^{-\frac{1}{2}}\m V\m R^{-\frac{1}{2}})\\
 & =\log\det(\m R)+\sum_{i=1}^{n}\log(1+t\lambda_{i}).
\end{aligned}
\end{equation}

\end_inset

where 
\begin_inset Formula $\lambda_{i}$
\end_inset

 are the eigenvalues of 
\begin_inset Formula $\m R^{-\frac{1}{2}}\m V\m R^{-\frac{1}{2}}$
\end_inset

.
 We can find that 
\begin_inset Formula $g(t)$
\end_inset

 is concave, hence 
\begin_inset Formula $\log\det(\m R)$
\end_inset

 is concave.
 While the last part is non-convex (code test).
 Hence the objective function in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:optimization"
plural "false"
caps "false"
noprefix "false"

\end_inset

) is non-convex in Euclidean spaces
\begin_inset CommandInset citation
LatexCommand cite
key "sun_robust_2016"
literal "false"

\end_inset

.
 However, from the perspective of geodesic convexity, the objective function
 can be viewed as g-convex, which will be proved in the next section.
\end_layout

\begin_layout Section
Convexity of Objective Function
\end_layout

\begin_layout Standard
First, we will introduce the definition of g-convex sets and the scalar
 g-convex functions.
\end_layout

\begin_layout Itemize
Definition 1 (g-convex sets).
 Let 
\begin_inset Formula $\mathcal{M}$
\end_inset

 be a d-dimensional connected 
\begin_inset Formula $C^{2}$
\end_inset

 Riemannian manifold.
 A set 
\begin_inset Formula $\mathcal{X}\subset\mathcal{M}$
\end_inset

 is called geodesically convex if any two points of 
\begin_inset Formula $\mathcal{X}$
\end_inset

 are joined by a geodesic lying in 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 That is, if 
\begin_inset Formula $x,y\in\mathcal{X},$
\end_inset

 then there exists a shortest path 
\begin_inset Formula $\gamma:[0,1]\rightarrow\mathcal{X}$
\end_inset

 such that 
\begin_inset Formula $\gamma(0)=x$
\end_inset

 and 
\begin_inset Formula $\gamma(1)=y$
\end_inset

 
\end_layout

\begin_layout Itemize
Definition 2 (g-convex functions).
 Let 
\begin_inset Formula $\mathcal{X}\subset\mathcal{M}$
\end_inset

 be a g-convex set.
 A function 
\begin_inset Formula $\phi:\mathcal{X}\rightarrow\mathbb{R}$
\end_inset

 is called geodesically convex if for any 
\begin_inset Formula $x,y\in\mathcal{X},$
\end_inset

 we have the inequality (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:g-convex"
plural "false"
caps "false"
noprefix "false"

\end_inset

) 
\begin_inset Formula 
\begin{equation}
\phi(\gamma(t))\leq(1-t)\phi(\gamma(0))+t\phi(\gamma(1))=(1-t)\phi(x)+t\phi(y).\label{eq:g-convex}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\gamma(\cdot)$
\end_inset

 is the geodesic 
\begin_inset Formula $\gamma:[0,1]\rightarrow\mathcal{X}$
\end_inset

 with 
\begin_inset Formula $\gamma(0)=x$
\end_inset

 and 
\begin_inset Formula $\gamma(1)=y$
\end_inset

.
\end_layout

\begin_layout Standard
To define g-convex functions on HPD matrices, we first define the 
\begin_inset Formula $\mathbb{P}_{d}$
\end_inset

 as a differentiable Riemannian manifold where geodesics between points
 are available in closed form.
 Recall the inner product 
\begin_inset Formula $\langle\mathbf{A}\text{,\ensuremath{\mathbf{B}}\ensuremath{\rangle=\text{tr}\mathbf{A}^{*}\mathbf{B}}}$
\end_inset

 and the associated norm 
\begin_inset Formula $\|\mathbf{A}\|_{2}=(\text{tr}\mathbf{A}^{*}\mathbf{A})^{\frac{1}{2}}$
\end_inset

 which leads to the Riemannian metric on 
\begin_inset Formula $\mathbb{P}_{d}$
\end_inset

—At any point 
\begin_inset Formula $A\in\mathbb{P}_{d}$
\end_inset

, its Riemannian metric is defined as 
\begin_inset Formula $ds=\|\m A^{\frac{1}{2}}d\m A\m A^{-\frac{1}{2}}\|_{2}=\left[\text{tr}(\mathbf{A}^{-1}d\mathbf{A})^{2}\right]^{\frac{1}{2}}$
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
after "6.1.6"
key "PositiveDefiniteMatrices"
literal "false"

\end_inset

.
 For 
\begin_inset Formula $\m A,\m B\in\mathbb{P}_{d}$
\end_inset

 there is a unique geodesic path joining 
\begin_inset Formula $\mathbf{A}$
\end_inset

 and 
\begin_inset Formula $\mathbf{B}$
\end_inset

.
 This geodesic has a parametrization
\begin_inset CommandInset citation
LatexCommand cite
key "sra_conic_2015"
literal "false"

\end_inset


\begin_inset Formula 
\begin{equation}
\gamma(t)=\m A\#_{t}\m B:=\m A^{\frac{1}{2}}(\m A^{-\frac{1}{2}}\m B\m A^{-\frac{1}{2}})^{t}\m A^{\frac{1}{2}},\qquad t\in[0,1].\label{eq:geodesic line}
\end{equation}

\end_inset

The midpoint of this path, namely, 
\begin_inset Formula $A\#_{1/2}B$
\end_inset

, is called the matrix geometric mean, we drop the 
\begin_inset Formula $1/2$
\end_inset

 and denote it simply by 
\begin_inset Formula $A\#B$
\end_inset

.
 
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "ANDO1979203"
literal "false"

\end_inset

, the map 
\begin_inset Formula $(\m A,\m B)\rightarrow\m A\#_{t}\m B$
\end_inset

, 
\begin_inset Formula $\m A,\m B\in\mathbb{P}_{d}$
\end_inset

 is concave.
 Then we have
\begin_inset Formula 
\begin{equation}
\m A\#_{t}\m B\preceq(1-t)\m A+t\m B.\label{eq:tracial}
\end{equation}

\end_inset

 (where 
\begin_inset Formula $\preceq$
\end_inset

 denotes the Löwner Partial order)
\end_layout

\begin_layout Standard
In order to prove the convexity of the objective function in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:optimization"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we firstly focus on the last part.
 We are going to prove that 
\begin_inset Formula $f(\m A)=\m x^{T}\m A^{-1}\m x$
\end_inset

 is g-convex.
 To prove this is equal to verify the midpoint convexity:
\begin_inset Formula 
\begin{equation}
f(\m A\#\m B)\le\frac{f(\m A)+f(\m B)}{2},
\end{equation}

\end_inset

where 
\begin_inset Formula $\m A,\m B\in\mathbb{P}_{d}$
\end_inset

.
 Since 
\begin_inset Formula $(\m A\#\m B)^{-1}=\m A^{-1}\#\m B^{-1}$
\end_inset

 and 
\begin_inset Formula $\m A^{-1}\#\m B^{-1}\preceq\frac{\m A^{-1}+\m B^{-1}}{2}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
after "4.16"
key "PositiveDefiniteMatrices"
literal "false"

\end_inset

 (the arithmetic-geometry mean inequality).
 Therefore
\begin_inset Formula 
\begin{equation}
f(\m A\#\m B)=\m x^{T}(\m A^{-1}\#\m B^{-1})\m x\le\m x^{T}\frac{\m A^{-1}+\m B^{-1}}{2}\m x=\frac{\m x^{T}\m A^{-1}\m x+\m x^{T}\m B^{-1}\m x}{2}=\frac{f(\m A)+f(\m B)}{2}.
\end{equation}

\end_inset

We prove convexity of the first part of objective function then,
\begin_inset Formula 
\begin{equation}
\begin{aligned}f(\m A\#\m B) & =\log\det(\m A^{\frac{1}{2}}(\m A^{-\frac{1}{2}}\m B\m A^{-\frac{1}{2}})^{t}\m A^{\frac{1}{2}})\\
 & =\log\det(\m A)+t\log\det(\m A^{-\frac{1}{2}}\m B\m A^{-\frac{1}{2}})\\
 & =(1-t)\log\det(\m A)+t\log\det(\m B)\\
 & =(1-t)f(\m A)+tf(\m B).
\end{aligned}
\end{equation}

\end_inset

If we take the logarithm to the second term, it is still g-convex.
 The proof is given below.
\end_layout

\begin_layout Proof
first we consider the eigenvalue decomposition
\begin_inset Formula 
\[
\m A^{-\frac{1}{2}}\m B\m A^{-\frac{1}{2}}=\m U\operatorname{diag}\{d_{j}\}\m U^{T},
\]

\end_inset

where 
\begin_inset Formula $d_{j}>0$
\end_inset

 are the eigenvalues and 
\begin_inset Formula $\m U$
\end_inset

 is the matrix of eigenvectors.
 We then consider the convexity of the second term in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:optimization"
plural "false"
caps "false"
noprefix "false"

\end_inset

)
\begin_inset Formula 
\[
\begin{aligned}\operatorname{log}(\m x_{i}^{T}(\m A\#_{t}\m B)^{-1}\m x_{i}) & =\operatorname{log}\m x_{i}^{T}\m A^{-\frac{1}{2}}(\m A^{-\frac{1}{2}}\m B\m A^{-\frac{1}{2}})^{-t}\m A^{-\frac{1}{2}}\m x_{i}\\
 & =\operatorname{log}\m x_{i}^{T}\m A^{-\frac{1}{2}}\m U\operatorname{diag}\{d_{j}^{-t}\}\m U^{T}\m A^{-\frac{1}{2}}\m x_{i}\\
 & =\operatorname{log}\sum_{j}\left(\m U^{T}\m A^{-\frac{1}{2}}\m x_{i}\right)_{j}^{2}d_{j}^{-t}\\
 & =\operatorname{log}\sum_{j}\left(\m U^{T}\m A^{-\frac{1}{2}}\m x_{i}\right)_{j}^{2}e^{-t\operatorname{log}(d_{j})}.
\end{aligned}
\]

\end_inset

The equation above is convex log-sum-exp expression in 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard
Therefore, the whole objective function is g-convex.
 Next we will introduce the useful theorem about g-convex.
\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $x^{*}\in\mathcal{M}$
\end_inset

 be a local minimum point of 
\begin_inset Formula $f:\mathcal{M\rightarrow\mathbb{R}}$
\end_inset

.
 If 
\begin_inset Formula $f$
\end_inset

 is convex, then 
\begin_inset Formula $x^{*}$
\end_inset

 is a global minimum point.
 
\begin_inset CommandInset citation
LatexCommand cite
key "udriste_convex_1994"
literal "false"

\end_inset


\end_layout

\begin_layout Proof
By hypothesis there exists 
\begin_inset Formula $\epsilon>0$
\end_inset

 such that
\begin_inset Formula 
\[
f(x^{*})\le f(x),\quad\forall x\in\{y|x^{*}-\epsilon\le y\le x^{*}+\epsilon,y\in\mathcal{M}\}.
\]

\end_inset

Suppose 
\begin_inset Formula $z\in\mathcal{M}$
\end_inset

 and 
\begin_inset Formula $f(z)<f(x^{*})$
\end_inset

.
 Consider the geodesic line from 
\begin_inset Formula $x^{*}$
\end_inset

 to 
\begin_inset Formula $z$
\end_inset

 denote as 
\begin_inset Formula $\gamma(t)$
\end_inset

, 
\begin_inset Formula $t\in(0,1)$
\end_inset

.
 Since 
\begin_inset Formula $f$
\end_inset

 is convex, it follows
\begin_inset Formula 
\[
f(\gamma(t))\le(1-t)f(x^{*})+tf(z)<f(x^{*}).
\]

\end_inset

But 
\begin_inset Formula $x=\gamma(t)\in\{y|x^{*}-\epsilon\le y\le x^{*}+\epsilon,y\in\mathcal{M}\},$
\end_inset

 for some 
\begin_inset Formula $t\in(0,1)$
\end_inset

 and hence 
\begin_inset Formula $f(x)\ge f(x^{*})$
\end_inset

, which is a contradiction and hence
\begin_inset Formula 
\[
f(z)\ge f(x^{*}),\quad\forall z\in\mathcal{M}.
\]

\end_inset


\end_layout

\begin_layout Standard
This result shows that general optimization algorithms can attain global
 solution.
 We will introduce several optimization algorithms in the following sections.
 
\end_layout

\begin_layout Section
Manifold Optimization
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\m X=[\m x_{1},\m x_{2},\ldots,\m x_{M}]$
\end_inset

, 
\begin_inset Formula $\m C=\m R^{-1}$
\end_inset

, and reformulate the objective function in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:optimization"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we have the new optimization problem below
\begin_inset Formula 
\begin{equation}
\begin{aligned} & \mathop{\text{minimize}}_{\mathbf{C}} &  & -\frac{1}{M}\log\det(\mathbf{C})+\text{tr}(\m X^{T}\m C\m X)\\
 & \mathop{\text{subject to}} &  & \mathbf{C}\in\text{HPD.}
\end{aligned}
\label{eq:optimization-2}
\end{equation}

\end_inset

For the objective function in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:optimization-2"
plural "false"
caps "false"
noprefix "false"

\end_inset

), the differential is given by
\begin_inset Formula 
\begin{equation}
\begin{aligned}Df(\m C)[\xi_{\m C}] & =-\frac{1}{M}\operatorname{tr}\left(\m C^{-1}\xi_{\m C}\right)+\operatorname{tr}(\m X\m X^{T}\xi_{\m C})\\
 & =\operatorname{tr}\left((\m X\m X^{T}-\m C^{-1}/M)\xi_{\m C}\right).
\end{aligned}
\label{eq:differential}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Inner product and gradient
\end_layout

\begin_layout Standard
Gradient-based optimization requires the notions of a gradient and an inner
 product, which will be introduced here for 
\begin_inset Formula $\mathbb{P}_{d}$
\end_inset

.
 The inner product most frequently associated with 
\begin_inset Formula $\mathbb{P}_{d}$
\end_inset

 is: for 
\begin_inset Formula $\xi_{\m C},\eta_{\m C}\in T_{\m C}\mathbb{P}_{d}$
\end_inset

, which is called intrinsic inner product, given by
\begin_inset Formula 
\begin{equation}
\langle\xi_{\m C},\eta_{\m C}\rangle_{\m C}=\operatorname{tr}(\xi_{\m C}\m C^{-1}\eta_{\m C}\m C^{-1}).\label{eq:inner product}
\end{equation}

\end_inset

Another inner product formulation called induced inner product was tested
 in 
\begin_inset CommandInset citation
LatexCommand cite
key "jeuris_survey_nodate"
literal "false"

\end_inset

 for 
\begin_inset Formula $\mathbb{P}_{d}$
\end_inset

.
 The induced inner product is given by
\begin_inset Formula 
\begin{equation}
\langle\xi_{\m C},\eta_{\m C}\rangle_{\m C}^{sym}=\operatorname{tr}(\xi_{\m C}\eta_{\m C}).\label{eq:induced inner product}
\end{equation}

\end_inset

The two inner product (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:inner product"
plural "false"
caps "false"
noprefix "false"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:induced inner product"
plural "false"
caps "false"
noprefix "false"

\end_inset

) deduce different geodesics.
 The intrinsic inner product leads to the geodesic (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:geodesic line"
plural "false"
caps "false"
noprefix "false"

\end_inset

) which is positive definite and monotone, however, the induced inner product
 leads to the geodesics below
\begin_inset Formula 
\begin{equation}
\gamma^{sym}(t)=\m A+t(\m B-\m A).
\end{equation}

\end_inset

these geodesics are no longer positive definite for all 
\begin_inset Formula $\m A,\m B,t$
\end_inset

 and thus not an element in 
\begin_inset Formula $\mathbb{P}_{d}$
\end_inset

.
 Whereas, for sufficiently small 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\gamma^{sym}(t)$
\end_inset

 is in 
\begin_inset Formula $\mathbb{P}_{d}$
\end_inset

.
 The authors in 
\begin_inset CommandInset citation
LatexCommand cite
key "jeuris_survey_nodate"
literal "false"

\end_inset

 observed that the intrinsic inner product leads to the better convergence
 speed for the tested algorithms than induced inner product.
 Therefore, we choose intrinsic inner product in our algorithm.
\end_layout

\begin_layout Standard
Furthermore, the gradient of the objective function gives the direction
 of steepest ascent.
 It can be defined at each point 
\begin_inset Formula $\m C$
\end_inset

 as the tangent vector 
\begin_inset Formula $\operatorname{grad}f(\m C)\in T_{\m C}M$
\end_inset

 such that
\begin_inset Formula 
\begin{equation}
\langle\operatorname{grad}f(\m C),\xi_{\m C}\rangle_{\m C}=Df(\m C)[\xi_{\m C}],\quad\forall\xi_{\m C}\in T_{\m C}M.
\end{equation}

\end_inset

Using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:differential"
plural "false"
caps "false"
noprefix "false"

\end_inset

) we find for our current setting when using the inner product in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:inner product"
plural "false"
caps "false"
noprefix "false"

\end_inset

)
\begin_inset Formula 
\begin{equation}
\operatorname{grad}f(\m C)=\m C(\m X\m X^{T}-\m C^{-1}/M)\m C\label{eq:grad}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Retraction and vector transport
\end_layout

\begin_layout Standard
Our optimization algorithms also require a map 
\begin_inset Formula $R_{\m C}:T_{\m C}\mathbb{P}_{d}\rightarrow\mathbb{P}_{d}$
\end_inset

 called retraction that locally maps 
\begin_inset Formula $T_{\m C}\mathbb{P}_{d}$
\end_inset

 onto the manifold 
\begin_inset Formula $\mathbb{P}_{d}$
\end_inset

 itself while preserving the first-order information of the tangent space
 in this point (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Retraction"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 If the manifold is Riemannian, a particular retraction is the exponential
 map, i.e., moving along a geodesic.
 The intrinsic inner product of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:inner product"
plural "false"
caps "false"
noprefix "false"

\end_inset

) of the Riemannian manifold leads to the following exponential map:
\begin_inset Formula 
\begin{equation}
R_{\m C}^{exp}(\xi_{\m C})=\m C^{\frac{1}{2}}e^{(\m C^{-\frac{1}{2}}\xi\m C^{-\frac{1}{2}})}\m C^{-\frac{1}{2}},\quad\xi\in T_{\m C}\mathbb{P}_{d}.\label{eq:exponential}
\end{equation}

\end_inset

Recall that the geodesic between 
\begin_inset Formula $\m A,\m B\in\mathbb{P}_{d}$
\end_inset

 is given by
\begin_inset Formula 
\begin{equation}
\begin{aligned}\gamma(t) & =\m A^{\frac{1}{2}}(\m A^{-\frac{1}{2}}\m B\m A^{-\frac{1}{2}})^{t}\m A^{\frac{1}{2}}\\
 & =\m A^{\frac{1}{2}}\operatorname{exp}\left(t\operatorname{log}(\m A^{-\frac{1}{2}}\m B\m A^{-\frac{1}{2}})\right)\m A^{\frac{1}{2}},\quad t\in[0,1].
\end{aligned}
\end{equation}

\end_inset

We obtain (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:exponential"
plural "false"
caps "false"
noprefix "false"

\end_inset

) as 
\begin_inset Formula $\gamma(t)$
\end_inset

 evaluated at 
\begin_inset Formula $t=1$
\end_inset

 with 
\begin_inset Formula $\xi_{\m C}=\m A^{\frac{1}{2}}\operatorname{log}(\m A^{-\frac{1}{2}}\m B\m A^{-\frac{1}{2}})\m A^{\frac{1}{2}}$
\end_inset

 and 
\begin_inset Formula $\m A=\m C$
\end_inset

.
 If we use the second order approximation which is given by
\begin_inset Formula 
\begin{equation}
e^{\m X}=\m I+\m X+\frac{1}{2}\m X^{2}+\mathcal{O}(\m X^{3}),\quad\m X\rightarrow0.
\end{equation}

\end_inset

 to substitute the retraction in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:exponential"
plural "false"
caps "false"
noprefix "false"

\end_inset

), we have
\begin_inset Formula 
\begin{equation}
R_{\m C}(\xi_{\m C})=\m C+\xi_{\m C}+\frac{1}{2}\xi_{\m C}C^{-1}\xi_{\m C}.
\end{equation}

\end_inset

From a numerical perspective, the retraction above gives a better computational
 speed.
\end_layout

\begin_layout Standard
Next, in order to perform, among others, the conjugate gradient algorithm,
 we need to somehow relate a tangent vector at some point 
\begin_inset Formula $\m X\in\mathbb{P}_{d}$
\end_inset

 to another point 
\begin_inset Formula $\m Y\in\mathbb{P}_{d}$
\end_inset

.
 This leads to the concept of a vector transport 
\begin_inset CommandInset citation
LatexCommand cite
after "10.5"
key "boumal_introduction_nodate"
literal "false"

\end_inset

 (see Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Vector-transport"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The vector transport for the intrinsic inner product is given by
\begin_inset Formula 
\begin{equation}
\mathcal{T}_{\m X,\m Y}(\xi_{\m X})=\m X^{\frac{1}{2}}(\m X^{-\frac{1}{2}}\m Y\m X^{-\frac{1}{2}})^{\frac{1}{2}}\m X^{-\frac{1}{2}}\xi_{\m X}\m X^{-\frac{1}{2}}(\m X^{-\frac{1}{2}}Y\m X^{-\frac{1}{2}})^{\frac{1}{2}}\m X^{\frac{1}{2}},\quad\xi_{\m X}\in T_{\m X}\mathbb{P}_{d}.
\end{equation}

\end_inset

this parallel transport can be written in a compact form that is also computatio
nally more advantageous, namely,
\begin_inset Formula 
\begin{equation}
\mathcal{T}_{\m X,\m Y}(\xi_{\m X})=E\xi_{\m X}E^{*},\quad\text{where }E=(\m Y\m X^{-1})^{\frac{1}{2}}.
\end{equation}

\end_inset

The elaborate statement about this vector transport is in 
\begin_inset CommandInset citation
LatexCommand cite
key "ferreira_newton_2006,jeuris_survey_nodate,sra_conic_2015"
literal "false"

\end_inset

.
\begin_inset Float figure
placement tbph
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figure1.pdf
	width 30page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Retraction"

\end_inset

Retraction
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset space \quad{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figure2.pdf
	width 30page%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Vector-transport"

\end_inset

Vector transport
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Simplified representations of a retraction and a vector transport
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Finite difference approximation of the Hessian
\end_layout

\begin_layout Standard
In order to minimize a smooth function on a Riemannian manifold, several
 optimization algorithms require computation of the Riemannian Hessian,
 
\begin_inset Formula $\operatorname{Hess}f(x)[\xi_{x}]$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Exact Hessian
\end_layout

\begin_layout Standard
The Riemannian Hessian of a real-valued function 
\begin_inset Formula $f$
\end_inset

 at a point 
\begin_inset Formula $\m C$
\end_inset

 on the manifold is a linear, symmetric mapping from the tangent space into
 itself given by
\begin_inset Formula 
\begin{equation}
\operatorname{Hess}f(\m C)[\xi_{\m C}]=\nabla_{\xi_{\m C}}\operatorname{grad}f,
\end{equation}

\end_inset

where 
\begin_inset Formula $\nabla$
\end_inset

 is the so-called Levi-Civita connection, which depends on the inner product,
 hence the Hessian will also depend on the inner product.
\end_layout

\begin_layout Standard
When endowed with the intrinsic inner product, the Levi-Civita connection
 is given by
\begin_inset Formula 
\begin{equation}
\nabla_{\zeta_{\m C}}\xi=D(\xi)(\m C)[\zeta_{\m C}]-\frac{1}{2}(\zeta_{\m C}\m C^{-1}\xi_{\m C}+\xi_{\m C}\m C^{-1}\zeta_{\m C}),
\end{equation}

\end_inset

which satisfies all properties of the Levi-Civita connection.
 Combine the equation above, the Hessian becomes
\begin_inset Formula 
\begin{equation}
\operatorname{Hess}f(\m C)[\xi_{\m C}]=D(\operatorname{grad}f)(\m C)[\xi_{\m C}]-\frac{1}{2}(\xi_{\m C}\m C^{-1}\operatorname{grad}f(\m C)+\operatorname{grad}f(\m C)\m C^{-1}\xi_{\m C}),
\end{equation}

\end_inset

In this case using (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:grad"
plural "false"
caps "false"
noprefix "false"

\end_inset

) leads to
\begin_inset Formula 
\begin{equation}
\operatorname{Hess}f(\m C)[\xi_{\m C}]=\frac{1}{2}\xi_{\m C}(\m X^{T}\m X-\m C^{-1}/M)\m C+\frac{1}{2}\m C(\m X^{T}\m X-\m C^{-1}/M)\xi_{\m C}+\xi_{\m C}/M.
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Hessian by decomposition
\end_layout

\begin_layout Standard
Another way to compute Hessian is described in 
\begin_inset CommandInset citation
LatexCommand cite
key "jeuris_survey_nodate,ferreira_newton_2013,ferreira_newton_2006,rentmeesters_algorithm_nodate"
literal "false"

\end_inset

.
 This method is established by exploiting some results about Jacobi fields
 together with the fact that the curvature endomorphism 
\begin_inset Formula $R$
\end_inset

 is parallel in these manifolds.
 The exact procedure consists 
\end_layout

\begin_layout Subsubsection
Hessian by approximation
\end_layout

\begin_layout Standard
As obtaining an explicit expression for the Hessian may be tedious, it is
 natural to avenues to approximate it numerically.
 To this end, we consider finite difference approximation.
\end_layout

\begin_layout Standard
For any smooth curve 
\begin_inset Formula $\gamma:I\rightarrow\mathcal{M}$
\end_inset

 such that 
\begin_inset Formula $\gamma(0)=x$
\end_inset

 and 
\begin_inset Formula $\gamma'(0)=\xi_{x}$
\end_inset

, it holds that
\begin_inset Formula 
\begin{equation}
\operatorname{Hess}f(x)[\xi_{x}]=\nabla_{\xi_{x}}\operatorname{grad}f=\frac{D}{dt}(\operatorname{grad}f(\gamma(0))),\label{eq:hess}
\end{equation}

\end_inset

where the 
\begin_inset Formula $\frac{D}{dt}$
\end_inset

 is the differential notation defined on vector transportation.
 The right-hand side in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hess"
plural "false"
caps "false"
noprefix "false"

\end_inset

) can be further rewrite as
\begin_inset Formula 
\begin{equation}
\operatorname{Hess}f(x)[\xi_{x}]=\lim_{t\rightarrow0}\frac{\text{\ensuremath{\mathcal{T}_{\gamma(t),0}(\operatorname{grad}f(\gamma(t)))}}-\operatorname{grad}f(x)}{t}.
\end{equation}

\end_inset

This suggests the approximation
\begin_inset Formula 
\begin{equation}
\operatorname{Hess}f(x)[\xi_{x}]\approx\frac{\text{\ensuremath{\mathcal{T}_{\gamma(t),0}(\operatorname{grad}f(\gamma(\hat{t})))}}-\operatorname{grad}f(x)}{\hat{t}},\label{eq:hess approx}
\end{equation}

\end_inset

for some well-chosen 
\begin_inset Formula $\hat{t}>0$
\end_inset

.
 Implementing this formula takes little effort compared to the hassle of
 deriving formulas for the Hessian directly (see numerical results in Figure
 ).
 In the Manopt toolbox, the default behavior when the Hessian is needed
 but unavailable is to fall back on (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:hess approx"
plural "false"
caps "false"
noprefix "false"

\end_inset

) with 
\begin_inset Formula $\gamma(\hat{t})=R_{x}(\hat{t}\xi_{x})$
\end_inset

 and 
\begin_inset Formula $\hat{t}>0$
\end_inset

 set such that 
\begin_inset Formula $\|\hat{t}\xi_{x}\|=2^{-14}$
\end_inset

.
 This costs one retraction, one gradient evaluation, and one call to a transport
er.
\end_layout

\begin_layout Section
Numerical Results
\end_layout

\begin_layout Standard
We now provide numerical examples of our algorithm.
 We first focus on the performance of different algorithm.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "covariance_estimation"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
