
@article{sra_conic_2015,
	title = {Conic {Geometric} {Optimization} on the {Manifold} of {Positive} {Definite} {Matrices}},
	volume = {25},
	issn = {1052-6234, 1095-7189},
	url = {http://epubs.siam.org/doi/10.1137/140978168},
	doi = {10.1137/140978168},
	abstract = {We develop geometric optimization on the manifold of Hermitian positive definite (HPD) matrices. In particular, we consider optimizing two types of cost functions: (i) geodesically convex (g-convex) and (ii) log-nonexpansive (LN). G-convex functions are nonconvex in the usual Euclidean sense but convex along the manifold and thus allow global optimization. LN functions may fail to be even g-convex but still remain globally optimizable due to their special structure. We develop theoretical tools to recognize and generate g-convex functions as well as cone theoretic fixedpoint optimization algorithms. We illustrate our techniques by applying them to maximum-likelihood parameter estimation for elliptically contoured distributions (a rich class that substantially generalizes the multivariate normal distribution). We compare our fixed-point algorithms with sophisticated manifold optimization methods and obtain notable speedups.},
	language = {en},
	number = {1},
	urldate = {2020-10-17},
	journal = {SIAM Journal on Optimization},
	author = {Sra, Suvrit and Hosseini, Reshad},
	month = jan,
	year = {2015},
	pages = {713--739},
	file = {Sra ? Hosseini - 2015 - Conic Geometric Optimization on the Manifold of Po.pdf:C\:\\Users\\sa\\Zotero\\storage\\RDIPGMT4\\Sra ? Hosseini - 2015 - Conic Geometric Optimization on the Manifold of Po.pdf:application/pdf}
}

@article{mishra_riemannian_2019,
	title = {Riemannian optimization on the simplex of positive definite matrices},
	url = {http://arxiv.org/abs/1906.10436},
	abstract = {We discuss optimization-related ingredients for the Riemannian manifold defined by the constraint X1 + X2 + . . . + XK = I, where the matrix Xi ? 0 is symmetric positive definite of size n {\texttimes} n for all i = \{1, . . . , K\}. For the case n = 1, the constraint boils down to the popular standard simplex constraint.},
	language = {en},
	urldate = {2020-10-17},
	journal = {arXiv:1906.10436 [cs, math]},
	author = {Mishra, Bamdev and Kasai, Hiroyuki and Jawanpuria, Pratik},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.10436},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Mishra ?? - 2019 - Riemannian optimization on the simplex of positive.pdf:C\:\\Users\\sa\\Zotero\\storage\\I9XM5QPE\\Mishra ?? - 2019 - Riemannian optimization on the simplex of positive.pdf:application/pdf}
}

@article{sun_robust_2016,
	title = {Robust {Estimation} of {Structured} {Covariance} {Matrix} for {Heavy}-{Tailed} {Elliptical} {Distributions}},
	volume = {64},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/7439863/},
	doi = {10.1109/TSP.2016.2546222},
	abstract = {This paper considers the problem of robustly estimating a structured covariance matrix with an elliptical underlying distribution with a known mean. In applications where the covariance matrix naturally possesses a certain structure, taking the prior structure information into account in the estimation procedure is beneficial to improving the estimation accuracy. We propose incorporating the prior structure information into Tyler{\textquoteright}s M-estimator and formulating the problem as minimizing the cost function of Tyler{\textquoteright}s estimator under the prior structural constraint. First, the estimation under a general convex structural constraint is introduced with an efficient algorithm for finding the estimator derived based on the majorization-minimization (MM) algorithm framework. Then, the algorithm is tailored to several special structures that enjoy a wide range of applications in signal processing related fields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitz structure. In addition, two types of non-convex structures, i.e., the Kronecker structure and the spiked covariance structure, are also discussed, where it is shown that simple algorithms can be derived under the guidelines of MM. The algorithms are guaranteed to converge to a stationary point of the problems. Furthermore, if the constraint set is geodesically convex, such as the Kronecker structure set, then the algorithm converges to a global minimum. Numerical results show that the proposed estimator achieves a smaller estimation error than the benchmark estimators at a lower computational cost.},
	language = {en},
	number = {14},
	urldate = {2020-10-17},
	journal = {IEEE Transactions on Signal Processing},
	author = {Sun, Ying and Babu, Prabhu and Palomar, Daniel P.},
	month = jul,
	year = {2016},
	pages = {3576--3590},
	file = {Sun ?? - 2016 - Robust Estimation of Structured Covariance Matrix .pdf:C\:\\Users\\sa\\Zotero\\storage\\BK953ZLS\\Sun ?? - 2016 - Robust Estimation of Structured Covariance Matrix .pdf:application/pdf}
}

@article{burg_estimation_1982,
	title = {Estimation of structured covariance matrices},
	volume = {70},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1456695/},
	doi = {10.1109/PROC.1982.12427},
	language = {en},
	number = {9},
	urldate = {2020-10-17},
	journal = {Proceedings of the IEEE},
	author = {Burg, J.P. and Luenberger, D.G. and Wenger, D.L.},
	year = {1982},
	pages = {963--974},
	file = {Burg ?? - 1982 - Estimation of structured covariance matrices.pdf:C\:\\Users\\sa\\Zotero\\storage\\HDK7G7ZJ\\Burg ?? - 1982 - Estimation of structured covariance matrices.pdf:application/pdf}
}

@article{wiesel_geodesic_2012,
	title = {Geodesic {Convexity} and {Covariance} {Estimation}},
	volume = {60},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/6298979/},
	doi = {10.1109/TSP.2012.2218241},
	abstract = {Geodesic convexity is a generalization of classical convexity which guarantees that all local minima of g-convex functions are globally optimal. We consider g-convex functions with positive definite matrix variables, and prove that Kronecker products, and logarithms of determinants are g-convex. We apply these results to two modern covariance estimation problems: robust estimation in scaled Gaussian distributions, and Kronecker structured models. Maximum likelihood estimation in these settings involves non-convex minimizations. We show that these problems are in fact g-convex. This leads to straight forward analysis, allows the use of standard optimization methods and paves the road to various extensions via additional g-convex regularization.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {IEEE Transactions on Signal Processing},
	author = {Wiesel, A.},
	month = dec,
	year = {2012},
	pages = {6182--6189},
	file = {Wiesel - 2012 - Geodesic Convexity and Covariance Estimation.pdf:C\:\\Users\\sa\\Zotero\\storage\\R75SPIFX\\Wiesel - 2012 - Geodesic Convexity and Covariance Estimation.pdf:application/pdf}
}

@article{wiesel_unified_2012,
	title = {Unified {Framework} to {Regularized} {Covariance} {Estimation} in {Scaled} {Gaussian} {Models}},
	volume = {60},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/6035802/},
	doi = {10.1109/TSP.2011.2170685},
	abstract = {We consider regularized covariance estimation in scaled Gaussian settings, e.g., elliptical distributions, compound-Gaussian processes and spherically invariant random vectors. Asymptotically in the number of samples, the classical maximum likelihood (ML) estimate is optimal under different criteria and can be efficiently computed even though the optimization is nonconvex. We propose a unified framework for regularizing this estimate in order to improve its finite sample performance. Our approach is based on the discovery of hidden convexity within the ML objective. We begin by restricting the attention to diagonal covariance matrices. Using a simple change of variables, we transform the problem into a convex optimization that can be efficiently solved. We then extend this idea to nondiagonal matrices using convexity on the manifold of positive definite matrices. We regularize the problem using appropriately convex penalties. These allow for shrinkage towards the identity matrix, shrinkage towards a diagonal matrix, shrinkage towards a given positive definite matrix, and regularization of the condition number. We demonstrate the advantages of these estimators using numerical simulations.},
	language = {en},
	number = {1},
	urldate = {2020-11-09},
	journal = {IEEE Transactions on Signal Processing},
	author = {Wiesel, Ami},
	month = jan,
	year = {2012},
	pages = {29--38},
	file = {Wiesel - 2012 - Unified Framework to Regularized Covariance Estima.pdf:C\:\\Users\\sa\\Zotero\\storage\\9YCI44SK\\Wiesel - 2012 - Unified Framework to Regularized Covariance Estima.pdf:application/pdf}
}

@article{wang_maximum_2006,
	title = {Maximum {Likelihood} {Estimation} of {Compound}-{Gaussian} {Clutter} and {Target} {Parameters}},
	volume = {54},
	issn = {1053-587X},
	url = {http://ieeexplore.ieee.org/document/1703856/},
	doi = {10.1109/TSP.2006.880209},
	abstract = {Compound-Gaussian models are used in radar signal processing to describe heavy-tailed clutter distributions. The important problems in compound-Gaussian clutter modeling are choosing the texture distribution, and estimating its parameters. Many texture distributions have been studied, and their parameters are typically estimated using statistically suboptimal approaches. We develop maximum likelihood (ML) methods for jointly estimating the target and clutter parameters in compound-Gaussian clutter using radar array measurements. In particular, we estimate i) the complex target amplitudes, ii) a spatial and temporal covariance matrix of the speckle component, and iii) texture distribution parameters. Parameter-expanded expectation{\textendash}maximization (PX-EM) algorithms are developed to compute the ML estimates of the unknown parameters. We also derived the Cram{\'e}r{\textendash}Rao bounds (CRBs) and related bounds for these parameters. We first derive general CRB expressions under an arbitrary texture model then simplify them for specific texture distributions. We consider the widely used gamma texture model, and propose an inverse-gamma texture model, leading to a complex multivariate clutter distribution and closed-form expressions of the CRB. We study the performance of the proposed methods via numerical simulations.},
	language = {en},
	number = {10},
	urldate = {2020-11-09},
	journal = {IEEE Transactions on Signal Processing},
	author = {Wang, J. and Dogandzic, A. and Nehorai, A.},
	month = oct,
	year = {2006},
	pages = {3884--3898},
	file = {Wang et al. - 2006 - Maximum Likelihood Estimation of Compound-Gaussian.pdf:C\:\\Users\\sa\\Zotero\\storage\\YQR8PAJ7\\Wang et al. - 2006 - Maximum Likelihood Estimation of Compound-Gaussian.pdf:application/pdf}
}

@article{bucciarelli_optimum_1996,
	title = {Optimum {CFAR} detection against compound {Gaussian} clutter with partially correlated texture},
	volume = {143},
	issn = {13502395},
	url = {https://digital-library.theiet.org/content/journals/10.1049/ip-rsn_19960248},
	doi = {10.1049/ip-rsn:19960248},
	abstract = {The paper deals with CFAR detection in compound Gaussian clutter with a partially correlated texture component. A theoretical high performance upgrade has been demonstrated using the ideal exact knowledge of this component in the CFAR scheme ({\textquoteleft}ideal CFAR{\textquoteright}) in a paper by Watts (1985) for K-distribution. For practical application the authors derive some optimum local texture estimators, based on the closest range cells, and use the estimated values to set the detection threshold. The schemes differ for operating over the intensity or the logarithm and for using or not prior information about the texture correlation. In particular, a maximum a posteriori estimator is derived, which outperforms the usual cell averaging CFAR and provides always performance close to the {\textquoteleft}ideal CFAR{\textquoteright}. The derivations are valid for all compound Gaussian clutters. The performances obtained with K and compound weibull distribution are compared, assessing the robustness of the proposed detection scheme.},
	language = {en},
	number = {2},
	urldate = {2020-11-09},
	journal = {IEE Proceedings - Radar, Sonar and Navigation},
	author = {Bucciarelli, T. and Lombardo, P. and Tamburrini, S.},
	year = {1996},
	pages = {95},
	file = {Bucciarelli et al. - 1996 - Optimum CFAR detection against compound Gaussian c.pdf:C\:\\Users\\sa\\Zotero\\storage\\I4PDEZCT\\Bucciarelli et al. - 1996 - Optimum CFAR detection against compound Gaussian c.pdf:application/pdf}
}

@article{chen_shrinkage_2010,
	title = {Shrinkage {Algorithms} for {MMSE} {Covariance} {Estimation}},
	volume = {58},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/5484583/},
	doi = {10.1109/TSP.2010.2053029},
	abstract = {We address covariance estimation in the sense of minimum mean-squared error (MMSE) when the samples are Gaussian distributed. Specifically, we consider shrinkage methods which are suitable for high dimensional problems with a small number of samples (large p small n). First, we improve on the Ledoit-Wolf (LW) method by conditioning on a sufficient statistic. By the Rao-Blackwell theorem, this yields a new estimator called RBLW, whose mean-squared error dominates that of LW for Gaussian variables. Second, to further reduce the estimation error, we propose an iterative approach which approximates the clairvoyant shrinkage estimator. Convergence of this iterative method is established and a closed form expression for the limit is determined, which is referred to as the oracle approximating shrinkage (OAS) estimator. Both RBLW and OAS estimators have simple expressions and are easily implemented. Although the two methods are developed from different perspectives, their structure is identical up to specified constants. The RBLW estimator provably dominates the LW method for Gaussian samples. Numerical simulations demonstrate that the OAS approach can perform even better than RBLW, especially when n is much less than p. We also demonstrate the performance of these techniques in the context of adaptive beamforming.},
	language = {en},
	number = {10},
	urldate = {2020-11-09},
	journal = {IEEE Transactions on Signal Processing},
	author = {Chen, Yilun and Wiesel, Ami and Eldar, Yonina C. and Hero, Alfred O.},
	month = oct,
	year = {2010},
	pages = {5016--5029},
	file = {Chen et al. - 2010 - Shrinkage Algorithms for MMSE Covariance Estimatio.pdf:C\:\\Users\\sa\\Zotero\\storage\\TB82W9BV\\Chen et al. - 2010 - Shrinkage Algorithms for MMSE Covariance Estimatio.pdf:application/pdf}
}

@article{schafer_shrinkage_2005,
	title = {A {Shrinkage} {Approach} to {Large}-{Scale} {Covariance} {Matrix} {Estimation} and {Implications} for {Functional} {Genomics}},
	volume = {4},
	issn = {1544-6115, 2194-6302},
	url = {https://www.degruyter.com/view/j/sagmb.2005.4.issue-1/sagmb.2005.4.1.1175/sagmb.2005.4.1.1175.xml},
	doi = {10.2202/1544-6115.1175},
	abstract = {Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are illsuited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity.},
	language = {en},
	number = {1},
	urldate = {2020-11-09},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Sch{\"a}fer, Juliane and Strimmer, Korbinian},
	month = jan,
	year = {2005},
	file = {Sch{\"a}fer and Strimmer - 2005 - A Shrinkage Approach to Large-Scale Covariance Mat.pdf:C\:\\Users\\sa\\Zotero\\storage\\RSNAKY7H\\Sch{\"a}fer and Strimmer - 2005 - A Shrinkage Approach to Large-Scale Covariance Mat.pdf:application/pdf}
}

@article{pascal_covariance_2008,
	title = {Covariance {Structure} {Maximum}-{Likelihood} {Estimates} in {Compound} {Gaussian} {Noise}: {Existence} and {Algorithm} {Analysis}},
	volume = {56},
	issn = {1053-587X, 1941-0476},
	shorttitle = {Covariance {Structure} {Maximum}-{Likelihood} {Estimates} in {Compound} {Gaussian} {Noise}},
	url = {http://ieeexplore.ieee.org/document/4359541/},
	doi = {10.1109/TSP.2007.901652},
	language = {en},
	number = {1},
	urldate = {2020-11-09},
	journal = {IEEE Transactions on Signal Processing},
	author = {Pascal, F. and Chitour, Y. and Ovarlez, J-P. and Forster, P. and Larzabal, P.},
	month = jan,
	year = {2008},
	pages = {34--48},
	file = {Pascal et al. - 2008 - Covariance Structure Maximum-Likelihood Estimates .pdf:C\:\\Users\\sa\\Zotero\\storage\\XG5HN45J\\Pascal et al. - 2008 - Covariance Structure Maximum-Likelihood Estimates .pdf:application/pdf}
}

@phdthesis{musolas2020covariance,
  title={Covariance estimation on matrix manifolds},
  author={Musolas Ota{\~n}o, Antoni Maria},
  year={2020},
  school={Massachusetts Institute of Technology}
}

@article{tyler_distribution-free_1987,
	title = {A {Distribution}-{Free} \${M}\$-{Estimator} of {Multivariate} {Scatter}},
	volume = {15},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1176350263},
	doi = {10.1214/aos/1176350263},
	language = {en},
	number = {1},
	urldate = {2020-12-11},
	journal = {The Annals of Statistics},
	author = {Tyler, David E.},
	month = mar,
	year = {1987},
	pages = {234--251},
	file = {Tyler - 1987 - A Distribution-Free \$M\$-Estimator of Multivariate .pdf:C\:\\Users\\DELL\\Zotero\\storage\\NDMJ6GKR\\Tyler - 1987 - A Distribution-Free \$M\$-Estimator of Multivariate .pdf:application/pdf}
}

@article{ANDO1979203,
title = "Concavity of certain maps on positive definite matrices and applications to Hadamard products",
journal = "Linear Algebra and its Applications",
volume = "26",
pages = "203 - 241",
year = "1979",
issn = "0024-3795",
doi = "https://doi.org/10.1016/0024-3795(79)90179-4",
url = "http://www.sciencedirect.com/science/article/pii/0024379579901794",
author = "T. Ando",
abstract = "If f is a positive function on (0, ∞) which is monotone of order n for every n in the sense of Löwner and if Φ1 and Φ2 are concave maps among positive definite matrices, then the following map involving tensor products: (A,B)↦f[Φ1(A)−1⊗Φ2(B)]·(Φ1(A)⊗I) is proved to be concave. If Φ1 is affine, it is proved without use of positivity that the map (A,B)↦f[Φ1(A)⊗Φ2(B)−1]·(Φ1(A)⊗I) is convex. These yield the concavity of the map (A,B)↦A1−p⊗Bp (0<p⩽1) (Lieb's theorem) and the convexity of the map (A,B)↦A1+p⊗B−p (0<p⩽1), as well as the convexity of the map (A,B)↦(A·log[A])⊗I−A⊗log[B]. These concavity and convexity theorems are then applied to obtain unusual estimates, from above and below, for Hadamard products of positive definite matrices."
}

@book { PositiveDefiniteMatrices,
      author = "Rajendra Bhatia",
      title = "Positive Definite Matrices",
      year = "10 Jan. 2009",
      publisher = "Princeton University Press",
      address = "Princeton",
      isbn = "978-1-4008-2778-7",
      doi = "https://doi.org/10.1515/9781400827787",
      url = "https://www.degruyter.com/princetonup/view/title/507014"
}