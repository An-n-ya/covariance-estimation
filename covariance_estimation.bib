
@book{boumal_introduction_nodate,
	title = {An introduction to optimization on smooth manifolds},
	language = {en},
	author = {Boumal, Nicolas},
	file = {Boumal - An introduction to optimization on smooth manifold.pdf:C\:\\Users\\DELL\\Zotero\\storage\\ZDNC25B8\\Boumal - An introduction to optimization on smooth manifold.pdf:application/pdf}
}

@article{sra_conic_2015,
	title = {Conic {Geometric} {Optimization} on the {Manifold} of {Positive} {Definite} {Matrices}},
	volume = {25},
	issn = {1052-6234, 1095-7189},
	url = {http://epubs.siam.org/doi/10.1137/140978168},
	doi = {10.1137/140978168},
	abstract = {We develop geometric optimization on the manifold of Hermitian positive definite (HPD) matrices. In particular, we consider optimizing two types of cost functions: (i) geodesically convex (g-convex) and (ii) log-nonexpansive (LN). G-convex functions are nonconvex in the usual Euclidean sense but convex along the manifold and thus allow global optimization. LN functions may fail to be even g-convex but still remain globally optimizable due to their special structure. We develop theoretical tools to recognize and generate g-convex functions as well as cone theoretic fixedpoint optimization algorithms. We illustrate our techniques by applying them to maximum-likelihood parameter estimation for elliptically contoured distributions (a rich class that substantially generalizes the multivariate normal distribution). We compare our fixed-point algorithms with sophisticated manifold optimization methods and obtain notable speedups.},
	language = {en},
	number = {1},
	urldate = {2020-10-17},
	journal = {SIAM Journal on Optimization},
	author = {Sra, Suvrit and Hosseini, Reshad},
	year = {2015},
	pages = {713--739},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\3YS9FRS8\\Sra ? Hosseini - 2015 - Conic Geometric Optimization on the Manifold of Po.pdf:application/pdf}
}

@article{mishra_riemannian_2019,
	title = {Riemannian optimization on the simplex of positive definite matrices},
	url = {http://arxiv.org/abs/1906.10436},
	abstract = {We discuss optimization-related ingredients for the Riemannian manifold defined by the constraint X1 + X2 + . . . + XK = I, where the matrix Xi ? 0 is symmetric positive definite of size n {\texttimes} n for all i = \{1, . . . , K\}. For the case n = 1, the constraint boils down to the popular standard simplex constraint.},
	language = {en},
	urldate = {2020-10-17},
	journal = {arXiv:1906.10436 [cs, math]},
	author = {Mishra, Bamdev and Kasai, Hiroyuki and Jawanpuria, Pratik},
	year = {2019},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\LVGHG6QR\\Mishra ?? - 2019 - Riemannian optimization on the simplex of positive.pdf:application/pdf}
}

@article{sun_robust_2016,
	title = {Robust {Estimation} of {Structured} {Covariance} {Matrix} for {Heavy}-{Tailed} {Elliptical} {Distributions}},
	volume = {64},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/7439863/},
	doi = {10.1109/TSP.2016.2546222},
	abstract = {This paper considers the problem of robustly estimating a structured covariance matrix with an elliptical underlying distribution with a known mean. In applications where the covariance matrix naturally possesses a certain structure, taking the prior structure information into account in the estimation procedure is beneficial to improving the estimation accuracy. We propose incorporating the prior structure information into Tyler{\textquoteright}s M-estimator and formulating the problem as minimizing the cost function of Tyler{\textquoteright}s estimator under the prior structural constraint. First, the estimation under a general convex structural constraint is introduced with an efficient algorithm for finding the estimator derived based on the majorization-minimization (MM) algorithm framework. Then, the algorithm is tailored to several special structures that enjoy a wide range of applications in signal processing related fields, namely, sum of rank-one matrices, Toeplitz, and banded Toeplitz structure. In addition, two types of non-convex structures, i.e., the Kronecker structure and the spiked covariance structure, are also discussed, where it is shown that simple algorithms can be derived under the guidelines of MM. The algorithms are guaranteed to converge to a stationary point of the problems. Furthermore, if the constraint set is geodesically convex, such as the Kronecker structure set, then the algorithm converges to a global minimum. Numerical results show that the proposed estimator achieves a smaller estimation error than the benchmark estimators at a lower computational cost.},
	language = {en},
	number = {14},
	urldate = {2020-10-17},
	journal = {IEEE Transactions on Signal Processing},
	author = {Sun, Ying and Babu, Prabhu and Palomar, Daniel P.},
	year = {2016},
	pages = {3576--3590},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\4Q8JJLUX\\Sun ?? - 2016 - Robust Estimation of Structured Covariance Matrix .pdf:application/pdf}
}

@article{burg_estimation_1982,
	title = {Estimation of structured covariance matrices},
	volume = {70},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1456695/},
	doi = {10.1109/PROC.1982.12427},
	language = {en},
	number = {9},
	urldate = {2020-10-17},
	journal = {Proceedings of the IEEE},
	author = {Burg, J.P. and Luenberger, D.G. and Wenger, D.L.},
	year = {1982},
	pages = {963--974},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\LUM6YTDT\\Burg ?? - 1982 - Estimation of structured covariance matrices.pdf:application/pdf}
}

@article{wiesel_geodesic_2012,
	title = {Geodesic {Convexity} and {Covariance} {Estimation}},
	volume = {60},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/6298979/},
	doi = {10.1109/TSP.2012.2218241},
	abstract = {Geodesic convexity is a generalization of classical convexity which guarantees that all local minima of g-convex functions are globally optimal. We consider g-convex functions with positive definite matrix variables, and prove that Kronecker products, and logarithms of determinants are g-convex. We apply these results to two modern covariance estimation problems: robust estimation in scaled Gaussian distributions, and Kronecker structured models. Maximum likelihood estimation in these settings involves non-convex minimizations. We show that these problems are in fact g-convex. This leads to straight forward analysis, allows the use of standard optimization methods and paves the road to various extensions via additional g-convex regularization.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {IEEE Transactions on Signal Processing},
	author = {Wiesel, A.},
	year = {2012},
	pages = {6182--6189},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\DM87XLQB\\Wiesel - 2012 - Geodesic Convexity and Covariance Estimation.pdf:application/pdf}
}

@article{wiesel_unified_2012,
	title = {Unified {Framework} to {Regularized} {Covariance} {Estimation} in {Scaled} {Gaussian} {Models}},
	volume = {60},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/6035802/},
	doi = {10.1109/TSP.2011.2170685},
	abstract = {We consider regularized covariance estimation in scaled Gaussian settings, e.g., elliptical distributions, compound-Gaussian processes and spherically invariant random vectors. Asymptotically in the number of samples, the classical maximum likelihood (ML) estimate is optimal under different criteria and can be efficiently computed even though the optimization is nonconvex. We propose a unified framework for regularizing this estimate in order to improve its finite sample performance. Our approach is based on the discovery of hidden convexity within the ML objective. We begin by restricting the attention to diagonal covariance matrices. Using a simple change of variables, we transform the problem into a convex optimization that can be efficiently solved. We then extend this idea to nondiagonal matrices using convexity on the manifold of positive definite matrices. We regularize the problem using appropriately convex penalties. These allow for shrinkage towards the identity matrix, shrinkage towards a diagonal matrix, shrinkage towards a given positive definite matrix, and regularization of the condition number. We demonstrate the advantages of these estimators using numerical simulations.},
	language = {en},
	number = {1},
	urldate = {2020-11-09},
	journal = {IEEE Transactions on Signal Processing},
	author = {Wiesel, Ami},
	year = {2012},
	pages = {29--38},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\AY628QM7\\Wiesel - 2012 - Unified Framework to Regularized Covariance Estima.pdf:application/pdf}
}

@article{wang_maximum_2006,
	title = {Maximum {Likelihood} {Estimation} of {Compound}-{Gaussian} {Clutter} and {Target} {Parameters}},
	volume = {54},
	issn = {1053-587X},
	url = {http://ieeexplore.ieee.org/document/1703856/},
	doi = {10.1109/TSP.2006.880209},
	abstract = {Compound-Gaussian models are used in radar signal processing to describe heavy-tailed clutter distributions. The important problems in compound-Gaussian clutter modeling are choosing the texture distribution, and estimating its parameters. Many texture distributions have been studied, and their parameters are typically estimated using statistically suboptimal approaches. We develop maximum likelihood (ML) methods for jointly estimating the target and clutter parameters in compound-Gaussian clutter using radar array measurements. In particular, we estimate i) the complex target amplitudes, ii) a spatial and temporal covariance matrix of the speckle component, and iii) texture distribution parameters. Parameter-expanded expectation{\textendash}maximization (PX-EM) algorithms are developed to compute the ML estimates of the unknown parameters. We also derived the Cram{\'e}r{\textendash}Rao bounds (CRBs) and related bounds for these parameters. We first derive general CRB expressions under an arbitrary texture model then simplify them for specific texture distributions. We consider the widely used gamma texture model, and propose an inverse-gamma texture model, leading to a complex multivariate clutter distribution and closed-form expressions of the CRB. We study the performance of the proposed methods via numerical simulations.},
	language = {en},
	number = {10},
	urldate = {2020-11-09},
	journal = {IEEE Transactions on Signal Processing},
	author = {Wang, J. and Dogandzic, A. and Nehorai, A.},
	year = {2006},
	pages = {3884--3898},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\RUDKH69N\\Wang ?? - 2006 - Maximum Likelihood Estimation of Compound-Gaussian.pdf:application/pdf}
}

@article{bucciarelli_optimum_1996,
	title = {Optimum {CFAR} detection against compound {Gaussian} clutter with partially correlated texture},
	volume = {143},
	issn = {13502395},
	url = {https://digital-library.theiet.org/content/journals/10.1049/ip-rsn_19960248},
	doi = {10.1049/ip-rsn:19960248},
	abstract = {The paper deals with CFAR detection in compound Gaussian clutter with a partially correlated texture component. A theoretical high performance upgrade has been demonstrated using the ideal exact knowledge of this component in the CFAR scheme ({\textquoteleft}ideal CFAR{\textquoteright}) in a paper by Watts (1985) for K-distribution. For practical application the authors derive some optimum local texture estimators, based on the closest range cells, and use the estimated values to set the detection threshold. The schemes differ for operating over the intensity or the logarithm and for using or not prior information about the texture correlation. In particular, a maximum a posteriori estimator is derived, which outperforms the usual cell averaging CFAR and provides always performance close to the {\textquoteleft}ideal CFAR{\textquoteright}. The derivations are valid for all compound Gaussian clutters. The performances obtained with K and compound weibull distribution are compared, assessing the robustness of the proposed detection scheme.},
	language = {en},
	number = {2},
	urldate = {2020-11-09},
	journal = {IEE Proceedings - Radar, Sonar and Navigation},
	author = {Bucciarelli, T. and Lombardo, P. and Tamburrini, S.},
	year = {1996},
	pages = {95}
}

@article{chen_shrinkage_2010,
	title = {Shrinkage {Algorithms} for {MMSE} {Covariance} {Estimation}},
	volume = {58},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/5484583/},
	doi = {10.1109/TSP.2010.2053029},
	abstract = {We address covariance estimation in the sense of minimum mean-squared error (MMSE) when the samples are Gaussian distributed. Specifically, we consider shrinkage methods which are suitable for high dimensional problems with a small number of samples (large p small n). First, we improve on the Ledoit-Wolf (LW) method by conditioning on a sufficient statistic. By the Rao-Blackwell theorem, this yields a new estimator called RBLW, whose mean-squared error dominates that of LW for Gaussian variables. Second, to further reduce the estimation error, we propose an iterative approach which approximates the clairvoyant shrinkage estimator. Convergence of this iterative method is established and a closed form expression for the limit is determined, which is referred to as the oracle approximating shrinkage (OAS) estimator. Both RBLW and OAS estimators have simple expressions and are easily implemented. Although the two methods are developed from different perspectives, their structure is identical up to specified constants. The RBLW estimator provably dominates the LW method for Gaussian samples. Numerical simulations demonstrate that the OAS approach can perform even better than RBLW, especially when n is much less than p. We also demonstrate the performance of these techniques in the context of adaptive beamforming.},
	language = {en},
	number = {10},
	urldate = {2020-11-09},
	journal = {IEEE Transactions on Signal Processing},
	author = {Chen, Yilun and Wiesel, Ami and Eldar, Yonina C. and Hero, Alfred O.},
	year = {2010},
	pages = {5016--5029},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\FK5APJRE\\Chen ?? - 2010 - Shrinkage Algorithms for MMSE Covariance Estimatio.pdf:application/pdf}
}

@article{schafer_shrinkage_2005,
	title = {A {Shrinkage} {Approach} to {Large}-{Scale} {Covariance} {Matrix} {Estimation} and {Implications} for {Functional} {Genomics}},
	volume = {4},
	issn = {1544-6115, 2194-6302},
	url = {https://www.degruyter.com/view/j/sagmb.2005.4.issue-1/sagmb.2005.4.1.1175/sagmb.2005.4.1.1175.xml},
	doi = {10.2202/1544-6115.1175},
	abstract = {Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are illsuited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity.},
	language = {en},
	number = {1},
	urldate = {2020-11-09},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Sch{\"a}fer, Juliane and Strimmer, Korbinian},
	year = {2005},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\AA6RVHAC\\Sch{\"a}fer ? Strimmer - 2005 - A Shrinkage Approach to Large-Scale Covariance Mat.pdf:application/pdf}
}

@article{pascal_covariance_2008,
	title = {Covariance {Structure} {Maximum}-{Likelihood} {Estimates} in {Compound} {Gaussian} {Noise}: {Existence} and {Algorithm} {Analysis}},
	volume = {56},
	issn = {1053-587X, 1941-0476},
	shorttitle = {Covariance {Structure} {Maximum}-{Likelihood} {Estimates} in {Compound} {Gaussian} {Noise}},
	url = {http://ieeexplore.ieee.org/document/4359541/},
	doi = {10.1109/TSP.2007.901652},
	language = {en},
	number = {1},
	urldate = {2020-11-09},
	journal = {IEEE Transactions on Signal Processing},
	author = {Pascal, F. and Chitour, Y. and Ovarlez, J-P. and Forster, P. and Larzabal, P.},
	year = {2008},
	pages = {34--48},
	file = {??:C\:\\Users\\DELL\\Zotero\\storage\\WRUNITDP\\Pascal ?? - 2008 - Covariance Structure Maximum-Likelihood Estimates .pdf:application/pdf}
}

@phdthesis{musolas_otano_covariance_2020,
	type = {{PhD} {Thesis}},
	title = {Covariance estimation on matrix manifolds},
	school = {Massachusetts Institute of Technology},
	author = {Musolas Ota{\~n}o, Antoni Maria},
	year = {2020},
	file = {Musolas Ota{\~n}o - 2020 - Covariance estimation on matrix manifolds.pdf:C\:\\Users\\DELL\\Zotero\\storage\\WBIP8NKC\\Musolas Ota{\~n}o - 2020 - Covariance estimation on matrix manifolds.pdf:application/pdf}
}

@article{pourahmadi_joint_1999,
	title = {Joint mean-covariance models with applications to longitudinal data: unconstrained parameterisation},
	volume = {86},
	issn = {0006-3444, 1464-3510},
	shorttitle = {Joint mean-covariance models with applications to longitudinal data},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/86.3.677},
	doi = {10.1093/biomet/86.3.677},
	abstract = {We provide unconstrained parameterisation for and model a covariance using covariates. The Cholesky decomposition of the inverse of a covariance matrix is used to associate a unique unit lower triangular and a unique diagonal matrix with each covariance matrix. The entries of the lower triangular and the log of the diagonal matrix are unconstrained and have meaning as regression coefficients and prediction variances when regressing a measurement on its predecessors. An extended generalised linear model is introduced for joint modelling of the vectors of predictors for the mean and covariance subsuming the joint modelling strategy for mean and variance heterogeneity, Gabriel's antedependence models, Dempster's covariance selection models and the class of graphical models. The likelihood function and maximum likelihood estimators of the covariance and the mean parameters are studied when the observations are normally distributed. Applications to modelling nonstationary dependence structures and multivariate data are discussed and illustrated using real data. A graphical method, similar to that based on the correlogram in time series, is developed and used to identify parametric models for nonstationary covariances.},
	language = {en},
	number = {3},
	urldate = {2020-11-13},
	journal = {Biometrika},
	author = {Pourahmadi, M},
	month = sep,
	year = {1999},
	keywords = {cholesky},
	pages = {677--690},
	file = {Pourahmadi - 1999 - Joint mean-covariance models with applications to .pdf:C\:\\Users\\DELL\\Zotero\\storage\\SKTN8ITA\\Pourahmadi - 1999 - Joint mean-covariance models with applications to .pdf:application/pdf}
}

@article{smith_covariance_2005,
	title = {Covariance, {Subspace}, and {Intrinsic} {Cram{\'e}r}{\textendash}{Rao} {Bounds}},
	volume = {53},
	language = {en},
	number = {5},
	journal = {IEEE TRANSACTIONS ON SIGNAL PROCESSING},
	author = {Smith, Steven Thomas},
	year = {2005},
	pages = {21},
	file = {Smith - 2005 - Covariance, Subspace, and Intrinsic Cram{\'e}r{\textendash}Rao Bou.pdf:C\:\\Users\\DELL\\Zotero\\storage\\LSJERW8Y\\Smith - 2005 - Covariance, Subspace, and Intrinsic Cram{\'e}r{\textendash}Rao Bou.pdf:application/pdf}
}

@article{musolas_geodesically_2020,
	title = {Geodesically parameterized covariance estimation},
	url = {http://arxiv.org/abs/2001.01805},
	abstract = {Statistical modeling of spatiotemporal phenomena often requires selecting a covariance matrix from a covariance class. Yet standard parametric covariance families can be insufficiently flexible for practical applications, while non-parametric approaches may not easily allow certain kinds of prior knowledge to be incorporated. We propose instead to build covariance families out of geodesic curves. These covariances offer more flexibility for problem-specific tailoring than classical parametric families, and are preferable to simple convex combinations. Once the covariance family has been chosen, one typically needs to select a representative member by solving an optimization problem, e.g., by maximizing the likelihood of a data set. We consider instead a differential geometric interpretation of this problem: minimizing the geodesic distance to a sample covariance matrix ({\textquotedblleft}natural projection{\textquotedblright}). Our approach is consistent with the notion of distance employed to build the covariance family and does not require assuming a particular probability distribution for the data. We show that natural projection and maximum likelihood are locally equivalent up to second order. We also demonstrate that natural projection may yield more accurate estimates with noise-corrupted data.},
	language = {en},
	urldate = {2020-12-09},
	journal = {arXiv:2001.01805 [math, stat]},
	author = {Musolas, Antoni and Smith, Steven T. and Marzouk, Youssef},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.01805},
	keywords = {Mathematics - Differential Geometry, Statistics - Computation},
	file = {Musolas ?? - 2020 - Geodesically parameterized covariance estimation.pdf:C\:\\Users\\DELL\\Zotero\\storage\\9SLK68HP\\Musolas ?? - 2020 - Geodesically parameterized covariance estimation.pdf:application/pdf}
}

@article{ledoit_well-conditioned_2004,
	title = {A well-conditioned estimator for large-dimensional covariance matrices},
	volume = {88},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X03000964},
	doi = {10.1016/S0047-259X(03)00096-4},
	abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For largedimensional covariance matrices, the usual estimator{\textemdash}the sample covariance matrix{\textemdash}is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.},
	language = {en},
	number = {2},
	urldate = {2020-12-10},
	journal = {Journal of Multivariate Analysis},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = feb,
	year = {2004},
	pages = {365--411},
	file = {Ledoit ? Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf:C\:\\Users\\DELL\\Zotero\\storage\\8GGBJT3N\\Ledoit ? Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf:application/pdf}
}

@article{chen_shrinkage_2010-1,
	title = {Shrinkage {Algorithms} for {MMSE} {Covariance} {Estimation}},
	volume = {58},
	issn = {1053-587X, 1941-0476},
	url = {http://ieeexplore.ieee.org/document/5484583/},
	doi = {10.1109/TSP.2010.2053029},
	abstract = {We address covariance estimation in the sense of minimum mean-squared error (MMSE) when the samples are Gaussian distributed. Specifically, we consider shrinkage methods which are suitable for high dimensional problems with a small number of samples (large p small n). First, we improve on the Ledoit-Wolf (LW) method by conditioning on a sufficient statistic. By the Rao-Blackwell theorem, this yields a new estimator called RBLW, whose mean-squared error dominates that of LW for Gaussian variables. Second, to further reduce the estimation error, we propose an iterative approach which approximates the clairvoyant shrinkage estimator. Convergence of this iterative method is established and a closed form expression for the limit is determined, which is referred to as the oracle approximating shrinkage (OAS) estimator. Both RBLW and OAS estimators have simple expressions and are easily implemented. Although the two methods are developed from different perspectives, their structure is identical up to specified constants. The RBLW estimator provably dominates the LW method for Gaussian samples. Numerical simulations demonstrate that the OAS approach can perform even better than RBLW, especially when n is much less than p. We also demonstrate the performance of these techniques in the context of adaptive beamforming.},
	language = {en},
	number = {10},
	urldate = {2020-12-10},
	journal = {IEEE Transactions on Signal Processing},
	author = {Chen, Yilun and Wiesel, Ami and Eldar, Yonina C. and Hero, Alfred O.},
	month = oct,
	year = {2010},
	pages = {5016--5029},
	file = {Chen ?? - 2010 - Shrinkage Algorithms for MMSE Covariance Estimatio.pdf:C\:\\Users\\DELL\\Zotero\\storage\\N8KLI9LI\\Chen ?? - 2010 - Shrinkage Algorithms for MMSE Covariance Estimatio.pdf:application/pdf}
}

@article{tyler_distribution-free_1987,
	title = {A {Distribution}-{Free} \${M}\$-{Estimator} of {Multivariate} {Scatter}},
	volume = {15},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1176350263},
	doi = {10.1214/aos/1176350263},
	language = {en},
	number = {1},
	urldate = {2020-12-11},
	journal = {The Annals of Statistics},
	author = {Tyler, David E.},
	month = mar,
	year = {1987},
	pages = {234--251},
	file = {Tyler - 1987 - A Distribution-Free \$M\$-Estimator of Multivariate .pdf:C\:\\Users\\DELL\\Zotero\\storage\\NDMJ6GKR\\Tyler - 1987 - A Distribution-Free \$M\$-Estimator of Multivariate .pdf:application/pdf}
}

@article{ando_concavity_1979,
	title = {Concavity of certain maps on positive definite matrices and applications to {Hadamard} products},
	volume = {26},
	issn = {0024-3795},
	url = {http://www.sciencedirect.com/science/article/pii/0024379579901794},
	doi = {https://doi.org/10.1016/0024-3795(79)90179-4},
	abstract = {If f is a positive function on (0, $\infty$) which is monotone of order n for every n in the sense of L{\"o}wner and if $\Phi$1 and $\Phi$2 are concave maps among positive definite matrices, then the following map involving tensor products: (A,B)?f[$\Phi$1(A)-1(x)$\Phi$2(B)]{\textperiodcentered}($\Phi$1(A)(x)I) is proved to be concave. If $\Phi$1 is affine, it is proved without use of positivity that the map (A,B)?f[$\Phi$1(A)(x)$\Phi$2(B)-1]{\textperiodcentered}($\Phi$1(A)(x)I) is convex. These yield the concavity of the map (A,B)?A1-p(x)Bp (0{\textless}p?1) (Lieb's theorem) and the convexity of the map (A,B)?A1+p(x)B-p (0{\textless}p?1), as well as the convexity of the map (A,B)?(A{\textperiodcentered}log[A])(x)I-A(x)log[B]. These concavity and convexity theorems are then applied to obtain unusual estimates, from above and below, for Hadamard products of positive definite matrices.},
	journal = {Linear Algebra and its Applications},
	author = {Ando, T.},
	year = {1979},
	pages = {203 -- 241},
	file = {1-s2.0-0024379579901794-main.pdf:C\:\\Users\\DELL\\Zotero\\storage\\46MP9Q9N\\1-s2.0-0024379579901794-main.pdf:application/pdf}
}

@article{auderset_angular_2005,
	title = {Angular {Gaussian} and {Cauchy} estimation},
	volume = {93},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X04000193},
	doi = {10.1016/j.jmva.2004.01.007},
	abstract = {This paper proposes a unified treatment of maximum likelihood estimates of angular Gaussian and multivariate Cauchy distributions in both the real and the complex case. The complex case is relevant in shape analysis. We describe in full generality the set of maxima of the corresponding log-likelihood functions with respect to an arbitrary probability measure. Our tools are the convexity of log-likelihood functions and their behaviour at infinity.},
	language = {en},
	number = {1},
	urldate = {2020-12-12},
	journal = {Journal of Multivariate Analysis},
	author = {Auderset, Claude and Mazza, Christian and Ruh, Ernst A.},
	month = mar,
	year = {2005},
	pages = {180--197},
	file = {Auderset ?? - 2005 - Angular Gaussian and Cauchy estimation.pdf:C\:\\Users\\DELL\\Zotero\\storage\\SE8W8V42\\Auderset ?? - 2005 - Angular Gaussian and Cauchy estimation.pdf:application/pdf}
}

@article{jeuris_survey_nodate,
	title = {A {SURVEY} {AND} {COMPARISON} {OF} {CONTEMPORARY} {ALGORITHMS} {FOR} {COMPUTING} {THE} {MATRIX} {GEOMETRIC} {MEAN}},
	abstract = {In this paper we present a survey of various algorithms for computing matrix geometric means and derive new second-order optimization algorithms to compute the Karcher mean. These new algorithms are constructed using the standard definition of the Riemannian Hessian. The survey includes the ALM list of desired properties for a geometric mean, the analytical expression for the mean of two matrices, algorithms based on the centroid computation in Euclidean (flat) space, and Riemannian optimization techniques to compute the Karcher mean (preceded by a short introduction into differential geometry). A change of metric is considered in the optimization techniques to reduce the complexity of the structures used in these algorithms. Numerical experiments are presented to compare the existing and the newly developed algorithms. We conclude that currently first-order algorithms are best suited for this optimization problem as the size and/or number of the matrices increase.},
	language = {en},
	author = {Jeuris, Ben and Vandebril, Raf and Vandereycken, Bart},
	pages = {24},
	file = {Jeuris ?? - A SURVEY AND COMPARISON OF CONTEMPORARY ALGORITHMS.pdf:C\:\\Users\\DELL\\Zotero\\storage\\4KPXPY6R\\Jeuris ?? - A SURVEY AND COMPARISON OF CONTEMPORARY ALGORITHMS.pdf:application/pdf}
}

@inproceedings{ferreira_newton_2006,
	address = {Toulouse, France},
	title = {Newton {Method} for {Riemannian} {Centroid} {Computation} in {Naturally} {Reductive} {Homogeneous} {Spaces}},
	volume = {3},
	isbn = {978-1-4244-0469-8},
	url = {http://ieeexplore.ieee.org/document/1660751/},
	doi = {10.1109/ICASSP.2006.1660751},
	abstract = {We address the problem of computing the Riemannian centroid of a constellation of points in a naturally reductive homogeneous manifold. We note that many interesting manifolds used in engineering (such as the special orthogonal group, Grassman, sphere, positive definite matrices) possess this structure. We develop an intrinsic Newton scheme for the centroid computation. This is achieved by exploiting a formula that we introduce for obtaining the Hessian of the squared Riemannian distance on naturally reductive homogeneous spaces. Some results of finding the centroid of a constellation of points in these spaces are presented, which evidence the quadratic convergence of the Newton method derived herein. These computer simulation results show that, as expected, the Newton method has a faster convergence rate than the usual gradient-based approaches.},
	language = {en},
	urldate = {2020-12-19},
	booktitle = {2006 {IEEE} {International} {Conference} on {Acoustics} {Speed} and {Signal} {Processing} {Proceedings}},
	publisher = {IEEE},
	author = {Ferreira, R. and Xavier, J. and Costeira, J.P. and Barroso, V.},
	year = {2006},
	pages = {III--704--III--707},
	file = {Ferreira ?? - 2006 - Newton Method for Riemannian Centroid Computation .pdf:C\:\\Users\\DELL\\Zotero\\storage\\WEJ4RGGI\\Ferreira ?? - 2006 - Newton Method for Riemannian Centroid Computation .pdf:application/pdf}
}

@article{journee_low-rank_2010,
	title = {Low-{Rank} {Optimization} on the {Cone} of {Positive} {Semidefinite} {Matrices}},
	volume = {20},
	issn = {1052-6234, 1095-7189},
	url = {http://epubs.siam.org/doi/10.1137/080731359},
	doi = {10.1137/080731359},
	abstract = {We propose an algorithm for solving optimization problems defined on a subset of the cone of symmetric positive semidefinite matrices. This algorithm relies on the factorization X = Y Y T , where the number of columns of Y fixes an upper bound on the rank of the positive semidefinite matrix X. It is thus very effective for solving problems that have a low-rank solution. The factorization X = Y Y T leads to a reformulation of the original problem as an optimization on a particular quotient manifold. The present paper discusses the geometry of that manifold and derives a second-order optimization method with guaranteed quadratic convergence. It furthermore provides some conditions on the rank of the factorization to ensure equivalence with the original problem. In contrast to existing methods, the proposed algorithm converges monotonically to the sought solution. Its numerical efficiency is evaluated on two applications: the maximal cut of a graph and the problem of sparse principal component analysis.},
	language = {en},
	number = {5},
	urldate = {2020-12-19},
	journal = {SIAM Journal on Optimization},
	author = {Journ{\'e}e, M. and Bach, F. and Absil, P.-A. and Sepulchre, R.},
	month = jan,
	year = {2010},
	pages = {2327--2351},
	file = {Journ{\'e}e ?? - 2010 - Low-Rank Optimization on the Cone of Positive Semi.pdf:C\:\\Users\\DELL\\Zotero\\storage\\LIW3W4YR\\Journ{\'e}e ?? - 2010 - Low-Rank Optimization on the Cone of Positive Semi.pdf:application/pdf}
}

@article{huang_riemannian_2018,
	title = {A {Riemannian} {BFGS} {Method} {Without} {Differentiated} {Retraction} for {Nonconvex} {Optimization} {Problems}},
	volume = {28},
	issn = {1052-6234, 1095-7189},
	url = {https://epubs.siam.org/doi/10.1137/17M1127582},
	doi = {10.1137/17M1127582},
	abstract = {In this paper, a Riemannian BFGS method for minimizing a smooth function on a Riemannian manifold is defined, based on a Riemannian generalization of a cautious update and a weak line search condition. It is proven that the Riemannian BFGS method converges (i) globally to stationary points without assuming the objective function to be convex and (ii) superlinearly to a nondegenerate minimizer. Using the weak line search condition removes the need for information from differentiated retraction. The joint matrix diagonalization problem is chosen to demonstrate the performance of the algorithms with various parameters, line search conditions, and pairs of retraction and vector transport. A preliminary version can be found in [Numerical Mathematics and Advanced Applications: ENUMATH 2015, Lect. Notes Comput. Sci. Eng. 112, Springer, New York, 2016, pp. 627{\textendash}634].},
	language = {en},
	number = {1},
	urldate = {2020-12-20},
	journal = {SIAM Journal on Optimization},
	author = {Huang, Wen and Absil, P.-A. and Gallivan, K. A.},
	month = jan,
	year = {2018},
	pages = {470--495},
	file = {Huang ?? - 2018 - A Riemannian BFGS Method Without Differentiated Re.pdf:C\:\\Users\\DELL\\Zotero\\storage\\WRR9AXK7\\Huang ?? - 2018 - A Riemannian BFGS Method Without Differentiated Re.pdf:application/pdf}
}

@book{udriste_convex_1994,
	address = {Dordrecht},
	title = {Convex {Functions} and {Optimization} {Methods} on {Riemannian} {Manifolds}},
	isbn = {978-90-481-4440-2 978-94-015-8390-9},
	url = {http://link.springer.com/10.1007/978-94-015-8390-9},
	language = {en},
	urldate = {2020-12-20},
	publisher = {Springer Netherlands},
	author = {Udri{\c s}te, Constantin},
	year = {1994},
	doi = {10.1007/978-94-015-8390-9},
	file = {Udri{\c s}te - 1994 - Convex Functions and Optimization Methods on Riema.pdf:C\:\\Users\\DELL\\Zotero\\storage\\8ZZ4DJDV\\Udri{\c s}te - 1994 - Convex Functions and Optimization Methods on Riema.pdf:application/pdf}
}

@article{ferreira_newton_2013,
	title = {Newton {Algorithms} for {Riemannian} {Distance} {Related} {Problems} on {Connected} {Locally} {Symmetric} {Manifolds}},
	volume = {7},
	issn = {1932-4553, 1941-0484},
	url = {http://ieeexplore.ieee.org/document/6514064/},
	doi = {10.1109/JSTSP.2013.2261799},
	abstract = {The squared distance function is one of the standard functions on which an optimization algorithm is commonly run, whether it is used directly or chained with other functions. Illustrative examples include center of mass computation, implementation of k-means algorithm and robot positioning. This function can have a simple expression (as in the Euclidean case), or it might not even have a closed form expression. Nonetheless, when used in an optimization problem formulated on non-Euclidean manifolds, the appropriate (intrinsic) version must be used and depending on the algorithm, its gradient and/or Hessian must be computed. For many commonly used manifolds a way to compute the intrinsic distance is available as well as its gradient, the Hessian however is usually a much more involved process, rendering Newton methods unusable on many standard manifolds. This article presents a way of computing the Hessian on connected locally-symmetric spaces on which standard Riemannian operations are known (exponential map, logarithm map and curvature). Although not a requirement for the result, describing the manifold as naturally reductive homogeneous spaces, a special class of manifolds, provides a way of computing these functions. The main example focused in this article is centroid computation of a finite constellation of points on connected locally symmetric manifolds since it is directly formulated as an intrinsic squared distance optimization problem. Simulation results shown here confirm the quadratic convergence rate of a Newton algorithm on commonly used manifolds such as the sphere, special orthogonal group, special Euclidean group, symmetric positive definite matrices, Grassmann manifold and projective space.},
	language = {en},
	number = {4},
	urldate = {2020-12-20},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Ferreira, Ricardo and Xavier, Joao and Costeira, Joao P. and Barroso, Victor},
	month = aug,
	year = {2013},
	pages = {634--645},
	file = {Ferreira ?? - 2013 - Newton Algorithms for Riemannian Distance Related .pdf:C\:\\Users\\DELL\\Zotero\\storage\\Y2Z8XAYB\\Ferreira ?? - 2013 - Newton Algorithms for Riemannian Distance Related .pdf:application/pdf}
}

@article{rentmeesters_algorithm_nodate,
	title = {Algorithm comparison for {Karcher} mean computation of rotation matrices and diffusion tensors},
	abstract = {This paper concerns the computation, by means of gradient and Newton methods, of the Karcher mean of a finite collection of points, both on the manifold of 3 {\texttimes} 3 rotation matrices endowed with its usual bi-invariant metric and on the manifold of 3 {\texttimes} 3 symmetric positive definite matrices endowed with its usual affine invariant metric. An explicit expression for the Hessian of the Riemannian squared distance function of these manifolds is given. From this, a condition on the step size of a constant step gradient method that depends on the data distribution is derived. These explicit expressions make a more efficient implementation of the Newton method possible and it is shown that the Newton method outperforms the gradient method in some cases.},
	language = {en},
	author = {Rentmeesters, Quentin},
	pages = {5},
	file = {Rentmeesters - Algorithm comparison for Karcher mean computation .pdf:C\:\\Users\\DELL\\Zotero\\storage\\IWPP7TQ2\\Rentmeesters - Algorithm comparison for Karcher mean computation .pdf:application/pdf}
}
